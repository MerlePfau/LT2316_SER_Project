{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile\n",
    "import os, pickle\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from glob import glob\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score\n",
    "import librosa\n",
    "from model import LSTM_fixed_len as model\n",
    "from model import Trainer\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up trainer\n",
    "model_dump = \"project_models/\" \n",
    "trainer = Trainer(device, dump_folder=model_dump) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_feature(file_name, mfcc, chroma, mel):\n",
    "#     with soundfile.SoundFile(file_name) as sound_file:\n",
    "#         X, sample_rate = librosa.load(file_name)\n",
    "#         if chroma:\n",
    "#             stft=np.abs(librosa.stft(X))\n",
    "#         result=np.array([])\n",
    "#         if mfcc:\n",
    "#             mfccs=np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T, axis=0)\n",
    "#             result=np.hstack((result, mfccs))\n",
    "#         if chroma:\n",
    "#             chroma=np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T,axis=0)\n",
    "#             result=np.hstack((result, chroma))\n",
    "#         if mel:\n",
    "#             mel=np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T,axis=0)\n",
    "#             result=np.hstack((result, mel))\n",
    "#     return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature(file_name):\n",
    "    #https://github.com/terranivium/speech-emotion-recognition/blob/master/speech_emotion_recognition.ipynb\n",
    "    with soundfile.SoundFile(file_name) as sound_file:\n",
    "        X = sound_file.read(dtype=\"float32\") # all depth 32bit float\n",
    "        sample_rate = sound_file.samplerate # always 16kHz\n",
    "        \n",
    "        # pre-emphasis\n",
    "        pre_emphasis = 0.97\n",
    "        X = np.append(X[0], X[1:] - pre_emphasis * X[:-1])\n",
    "\n",
    "        # remove silence\n",
    "        y = librosa.effects.split(X, top_db=20)\n",
    "        l = []\n",
    "        for i in y:\n",
    "            l.append(X[i[0]:i[1]] )\n",
    "        X = np.concatenate(l, axis=0)\n",
    "        \n",
    "        # extract features\n",
    "        hop_length=int(0.100*sample_rate)\n",
    "        n_fft=int(0.500*sample_rate)\n",
    "        mfccs = librosa.feature.mfcc(y=X, \n",
    "                                     sr=sample_rate, \n",
    "                                     n_mfcc=40, \n",
    "                                     n_mels=40,\n",
    "                                     power=2.0,\n",
    "                                     window = 'hamming',\n",
    "                                     fmin = 0,\n",
    "                                     fmax = 8000,\n",
    "                                     hop_length=hop_length,\n",
    "                                     n_fft=n_fft,\n",
    "                                     win_length=n_fft,\n",
    "                                     center=True)  \n",
    "    return mfccs[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions={\n",
    "  0:'neutral',\n",
    "  1:'calm',\n",
    "  2:'happy',\n",
    "  3:'sad',\n",
    "  4:'angry',\n",
    "  5:'fearful',\n",
    "  6:'disgust',\n",
    "  7:'surprised'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    X,y=[],[]\n",
    "    all_files = Path(\"Audio_Speech_Actors_01-24\")\n",
    "    i = 0\n",
    "    for file in all_files.glob(\"Actor_*/*.wav\"):\n",
    "#         file_name=os.path.basename(file)\n",
    "#         emotion = file_name.split(\"-\")[2]\n",
    "#         emotion = emotion.replace('0','')\n",
    "#         emotion = int(emotion)-1\n",
    "#         #feature=extract_feature(file, mfcc=True, chroma=True, mel=True)\n",
    "#         feature=extract_feature(file)\n",
    "#         #print(feature.shape, type(feature))\n",
    "#         x.append(feature)\n",
    "#         y.append(emotion)\n",
    "# #         i += 1\n",
    "# #         if i > 200:\n",
    "# #             break\n",
    "    \n",
    "    \n",
    "        # get the base name of the audio file\n",
    "        basename = os.path.basename(file)\n",
    "        # get the emotion label\n",
    "        emotion = basename.split(\"-\")[2]\n",
    "        emotion = emotion.replace('0','')\n",
    "        emotion = int(emotion)-1\n",
    "        # extract features\n",
    "        features = extract_feature(file)\n",
    "        # add to data\n",
    "        X.append(features)\n",
    "#         mul_circumplex_coord = [emotion] * features.shape[1]\n",
    "#         y.extend(mul_circumplex_coord)\n",
    "        y.append(emotion)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guspfame@GU.GU.SE/.local/lib/python3.7/site-packages/librosa/core/spectrum.py:224: UserWarning: n_fft=24000 is too small for input signal of length=22528\n",
      "  n_fft, y.shape[-1]\n",
      "/home/guspfame@GU.GU.SE/.local/lib/python3.7/site-packages/librosa/core/spectrum.py:224: UserWarning: n_fft=24000 is too small for input signal of length=22016\n",
      "  n_fft, y.shape[-1]\n",
      "/home/guspfame@GU.GU.SE/.local/lib/python3.7/site-packages/librosa/core/spectrum.py:224: UserWarning: n_fft=24000 is too small for input signal of length=11264\n",
      "  n_fft, y.shape[-1]\n",
      "/home/guspfame@GU.GU.SE/.local/lib/python3.7/site-packages/librosa/core/spectrum.py:224: UserWarning: n_fft=24000 is too small for input signal of length=13312\n",
      "  n_fft, y.shape[-1]\n",
      "/home/guspfame@GU.GU.SE/.local/lib/python3.7/site-packages/librosa/core/spectrum.py:224: UserWarning: n_fft=24000 is too small for input signal of length=12288\n",
      "  n_fft, y.shape[-1]\n",
      "/home/guspfame@GU.GU.SE/.local/lib/python3.7/site-packages/librosa/core/spectrum.py:224: UserWarning: n_fft=24000 is too small for input signal of length=20992\n",
      "  n_fft, y.shape[-1]\n",
      "/home/guspfame@GU.GU.SE/.local/lib/python3.7/site-packages/librosa/core/spectrum.py:224: UserWarning: n_fft=24000 is too small for input signal of length=19968\n",
      "  n_fft, y.shape[-1]\n",
      "/home/guspfame@GU.GU.SE/.local/lib/python3.7/site-packages/librosa/core/spectrum.py:224: UserWarning: n_fft=24000 is too small for input signal of length=18944\n",
      "  n_fft, y.shape[-1]\n",
      "/home/guspfame@GU.GU.SE/.local/lib/python3.7/site-packages/librosa/core/spectrum.py:224: UserWarning: n_fft=24000 is too small for input signal of length=15872\n",
      "  n_fft, y.shape[-1]\n",
      "/home/guspfame@GU.GU.SE/.local/lib/python3.7/site-packages/librosa/core/spectrum.py:224: UserWarning: n_fft=24000 is too small for input signal of length=16384\n",
      "  n_fft, y.shape[-1]\n",
      "/home/guspfame@GU.GU.SE/.local/lib/python3.7/site-packages/librosa/core/spectrum.py:224: UserWarning: n_fft=24000 is too small for input signal of length=21504\n",
      "  n_fft, y.shape[-1]\n",
      "/home/guspfame@GU.GU.SE/.local/lib/python3.7/site-packages/librosa/core/spectrum.py:224: UserWarning: n_fft=24000 is too small for input signal of length=23040\n",
      "  n_fft, y.shape[-1]\n",
      "/home/guspfame@GU.GU.SE/.local/lib/python3.7/site-packages/librosa/core/spectrum.py:224: UserWarning: n_fft=24000 is too small for input signal of length=17920\n",
      "  n_fft, y.shape[-1]\n",
      "/home/guspfame@GU.GU.SE/.local/lib/python3.7/site-packages/librosa/core/spectrum.py:224: UserWarning: n_fft=24000 is too small for input signal of length=14336\n",
      "  n_fft, y.shape[-1]\n",
      "/home/guspfame@GU.GU.SE/.local/lib/python3.7/site-packages/librosa/core/spectrum.py:224: UserWarning: n_fft=24000 is too small for input signal of length=23552\n",
      "  n_fft, y.shape[-1]\n",
      "/home/guspfame@GU.GU.SE/.local/lib/python3.7/site-packages/librosa/core/spectrum.py:224: UserWarning: n_fft=24000 is too small for input signal of length=16896\n",
      "  n_fft, y.shape[-1]\n",
      "/home/guspfame@GU.GU.SE/.local/lib/python3.7/site-packages/librosa/core/spectrum.py:224: UserWarning: n_fft=24000 is too small for input signal of length=20480\n",
      "  n_fft, y.shape[-1]\n",
      "/home/guspfame@GU.GU.SE/.local/lib/python3.7/site-packages/librosa/core/spectrum.py:224: UserWarning: n_fft=24000 is too small for input signal of length=17408\n",
      "  n_fft, y.shape[-1]\n",
      "/home/guspfame@GU.GU.SE/.local/lib/python3.7/site-packages/librosa/core/spectrum.py:224: UserWarning: n_fft=24000 is too small for input signal of length=15360\n",
      "  n_fft, y.shape[-1]\n",
      "/home/guspfame@GU.GU.SE/.local/lib/python3.7/site-packages/librosa/core/spectrum.py:224: UserWarning: n_fft=24000 is too small for input signal of length=10240\n",
      "  n_fft, y.shape[-1]\n"
     ]
    }
   ],
   "source": [
    "x, y = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 0\n",
    "for row in x:\n",
    "    for features in row:\n",
    "        if len(features) > max_len:\n",
    "            max_len = len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "for row in x:\n",
    "    rowX = []\n",
    "    for features in row:\n",
    "        padding = [0] * (max_len-len(features))\n",
    "        features = list(features)\n",
    "        features.extend(padding)\n",
    "        rowX.append(features)\n",
    "    X.append(rowX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1440 39 26\n"
     ]
    }
   ],
   "source": [
    "print(len(X),len(X[0]), len(X[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1440"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.Tensor(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=9)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.25, random_state=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, x_val, y_train, y_test, y_val = torch.Tensor(x_train), torch.Tensor(x_test), torch.Tensor(x_val), torch.LongTensor(y_train),  torch.LongTensor(y_test), torch.LongTensor(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, x_val =  x_train.to(device), x_test.to(device), x_val.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, y_test, y_val =y_train.to(device), y_test.to(device), y_val.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train,x_test,y_train,y_test = x_train.to(device), x_test.to(device), y_train.to(device), y_test.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([864, 39, 26])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features extracted: 39\n"
     ]
    }
   ],
   "source": [
    "print(f'Features extracted: {x_train.shape[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_size=len(list(emotions.keys()))\n",
    "\n",
    "set_hyperparameters = [{\"learning_rate\": 0.001,\n",
    "                        \"hidden_size\": 100,\n",
    "                        \"number_layers\": 3,\n",
    "                        \"batch_size\": 25,\n",
    "                        \"model\": 'model1'\n",
    "                        },\n",
    "                       {\"learning_rate\": 0.002,\n",
    "                        \"hidden_size\": 300,\n",
    "                        \"number_layers\": 5,\n",
    "                        \"batch_size\": 100,\n",
    "                        \"model\": 'model2'\n",
    "                       },\n",
    "                       {\"learning_rate\": 0.001,\n",
    "                        \"hidden_size\": 32,\n",
    "                        \"number_layers\": 10,\n",
    "                        \"batch_size\": 80,\n",
    "                        \"model\": 'model3'\n",
    "                       },\n",
    "                       {\"learning_rate\": 0.0005,\n",
    "                        \"hidden_size\": 100,\n",
    "                        \"number_layers\": 3,\n",
    "                        \"batch_size\": 25,\n",
    "                        \"model\": 'model4'\n",
    "                       },\n",
    "                       {\"learning_rate\": 0.001,\n",
    "                        \"hidden_size\": 250,\n",
    "                        \"number_layers\": 3,\n",
    "                        \"batch_size\": 32,\n",
    "                        \"model\": 'model5'\n",
    "                       }\n",
    "                      ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for hp in set_hyperparameters:\n",
    "    trainer.train_model(model, x_train, y_train, x_val, y_val, hp, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_path = \"project_models/model5.pt\"\n",
    "y_pred = trainer.predict(x_test, model, best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = f1_score(y_test, y_pred, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"F1 Score: {:.2f}%\".format(f*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
